1. 参数共享指的是什么？
	对于不同的数据，同一个卷积核上的参数值是相同的

2. 为什么会用到batch normalization ?
	BN就是通过一定的规范化手段，把每层神经网络任意神经元　这个输入值的分布强行拉回到均值为0方差为1的标准正态分布；非线性变换函数的输入值落入对输入比较敏感的区域，从而避免梯度消失问题。这样输入的小变化就会导致损失函数较大的变化（使得梯度变大，避免梯度消失问题产生），同时也让收敛速度更快，加快训练速度

3. 使用dropout可以解决什么问题？
	随机闭合部分神经元，防止训练过拟合


原理	传统图像识别模型
	数据获取：通过图像输入设备实现。
	预处理：提高图像质量，包括滤波、平滑、增强、复原、提取边缘、图像分割等方法
	特征提取和选择：将预处理后的图像转化为若干特征（常见特征有：幅度特征，统计特征，几何特征，变换系数特征等）

原理	CNN的由来
	卷积神经网络（CNN）是人工神经网络的一种，是多层感知机（MLP）的一个变种模型，它是从生物学概念中演化而来的。

原理	局部感受野，共享权重，池化
	1. 局部感受野：图像的空间联系是局部的，就像人通过局部的感受野去感受外界图像一样，每个神经元只感受局部的图像区域，然后在更高层，将这些感受不同局部的神经元综合起来就可以得到全局的信息了。
	CNN中相邻层之间是部分连接，即某个神经单元的感知区域来自于上层的部分神经单元
	2. 共享权重：隐层的参数个数和隐层的神经元个数无关，只和滤波器的大小和滤波器种类的多少有关
	3. 池化：根据图像局部相关的原理，图像某个邻域内只需要一个像素点就能表达整个区域的信息，也称为混合、下采样。


原理	平移不变与空间不变特性

原理	卷积核的作用
	特征提取

原理	Batch Normalization
	BN就是通过一定的规范化手段，把每层神经网络任意神经元　这个输入值的分布强行拉回到均值为0方差为1的标准正态分布
	使得非线性变换函数的输入值落入对输入比较敏感的区域，从而避免梯度消失问题。这样输入的小变化就会导致损失函数较大的变化（使得梯度变大，避免梯度消失问题产生），同时也让收敛速度更快，加快训练速度


原理	Pooling与Dropout
	

工具	AlexNet原理及使用
工具	ResNet原理及使用
原理	DenseNet原理
